**Task 2: Random Labels**

    Training Bayesian Neural Networks (BNNs):
    
    In this step, we train Bayesian Neural Networks (BNNs) on the MNIST dataset. These networks are used to recognize and classify handwritten digits. Comparison with and without Random Labels:
    
    Next, we compare the performance of these BNNs with and without a random label assignment process. The random label assignment involves assigning labels to the data randomly. KL Divergence Calculation:
    
    We calculate the Kullback-Leibler (KL) divergence values between the prior and posterior distributions for each model. KL divergence measures the difference between two probability distributions. Discussion of Results:
    
    Finally, we discuss and analyze the results in our report. We examine how random label assignment impacts the performance of Bayesian Neural Networks and provide insights into the findings.

**Task 3: L2 Regularization and Bayesian Neural Networks**

    Training Different Models:
    
    In this task, we train various machine learning models on the MNIST dataset. We start with a logistic regression model, which is a basic classifier. We also train a logistic regression model with L2 regularization, which helps prevent overfitting. Additionally, we experiment with Bayesian Neural Networks, exploring variations in the number of layers and hidden sizes. Model Comparison:
    
    We compare the performance of these different models in terms of accuracy and architecture. We also consider the theoretical and practical implications of using these models.
  
**Documentation and Reporting:**
  
We document the model architectures, training procedures, and convergence plots. The convergence plots show how the models' accuracy evolves over training epochs. We summarize our findings and insights in our report. In both tasks, we conduct experiments, analyze the results, and provide detailed documentation and reporting as part of our machine learning analysis.
